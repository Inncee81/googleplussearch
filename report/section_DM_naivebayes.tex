\subsection{Naïve Bayes Classifier}
\textbf{Student Name: }Alexander Nieuwenhuijse \textbf{Student ID:} 0744933\\
This subsection describes the technique used for building a naïve bayes classifier and how it was used within the project.
\subsubsection*{Motivation}
We want to be able to classify posts based on topics such as “technology”, “Entertainment” etc. This can then be used to query for posts with the same topic and help disambiguation of a user query.
\subsubsection*{Problem formulation}
To be able to classify posts into a predefined set of tasks we use training data generated by the crawler and use this data to train a naïve bayes classifier. This classifier can then be used to determine topics for a user query and help with the building of an index on the data generated by the crawler.
\subsubsection*{Approach}
The technique used for building a Naïve Bayes classifier is relatively easy when compared to other classification techniques, and it classifies its input very reliably. It is a probabilistic classification technique that, given a set of attributes and classes, will determine which class has the highest probability of describing an input document based on its attributes. It is important to note that this technique assumes independence between the attributes for every class.
The naïve bayes classifier classifies a document that has the highest probability:$ P(C|Document-terms)$. Using Bayes theorem this is equal to:
$$P(C|Document-terms)= \frac{P(Document-terms|C)P(C)}{P(Document-terms)}$$ 
To estimate the value of $P(Document-terms|C)P(C)$ we use a labeled training set of data which contains a set of Document-terms and their corresponding class. Given this training set (T) we estimate the following probability:
\begin{eqnarray*}
P(C|Document-terms) & = & P(T_{1}|C)\cdot P(T_{2}|C)\cdot\ldots\cdot P(T_{n}|C)\\
 & = & \sum_{i=1}^{n}P(T_{i}|C)\:\text{}\{Assuming\, independence\}
\end{eqnarray*}
The probability of $P(T_{i}|C)$ is determined as follows: 
$$P(T_{i}|C)=\frac{T_{ct}}{\sum_{t'\in V}T_{ct'}}$$ 
where $T_{ct}$ is the total number of occurrences of the term $T_{i}$ in the set of training documents with class $C$.
We can estimate $P(C)$ using the following estimator: $$P(C) = \frac{N_{c}}{N}$$ where $N_{c}$ is the number of documents with class $C$ in the total training set.
Using these probabilities we can determine the class which has highest probability of a document belonging in that class.