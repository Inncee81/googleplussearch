\subsection{Crawler}
\textbf{Student Name: }Alexander Nieuwenhuijse \textbf{Student ID:} 0744933\\
In this subsection a description is given of the crawler that was created to retrieve Google+ posts, comments and user profile data. We start by motivating the use of the application programmable interface(API) provided by the Google+ followed by the implementation details of the crawler program and finish by showing the generated output that is used later on in the pipeline.
\subsubsection*{Motivation}
At the start of the project we decided that we wanted to make an offline search engine for posts, comments and user profiles on Google+. To realise this we need a large collection of data to fill our database such that most queries made by users will have corresponding documents in our database. It will not be possible for us to cover the entire Google+ collection of data due to time constraints and limitations on the interface used to retrieve documents. For the retrieval of documents we looked into two different sources: The Google+ API and the manually parsing of the Google+ web pages. 
\subsubsection*{Problem formulation}
We want to retrieve post information and user profile data given a set of keywords or a specific Google+ community page and store this data in an easy to use format for the training of classifiers and to allow the building of an index on this data.
\subsubsection*{Approach}
When comparing both data sources we asked ourselves the following questions: "How much information does the source provide?", "How accurate is the provided data?", "What are the limitations of using this source?", "How much time does it take to mine the data?". When looking at the API we first noticed that it was very well documented and provided a very easy to learn interface. The result from querying the API was formatted in JSON* which made parsing of the data almost trivial. The complete opposite was true when manually parsing the Google+ web pages. When looking at the source-code of a webpage displaying a list of posts made by a user we noticed a large piece of code that contained all the data of the posts displayed on the website, and some additional posts not shown on the website. These additional posts were part of some caching technique used by Google and would allow us to retrieve a lot more data from a single web requests than the API would. However, the problem with parsing the pages manually comes from the formatting, since the data cache is stored in an unknown JSON-like format without any specification. To use this data we would have to reverse-engineer the meaning of the data, which would be a very time consuming process. A comparison of the two data formats is given in Appendix XXX. 
Another major difference between the two sources is the accessibility. The API has restricted the number of requests allowed per day to 10,000 per user whereas the manually crawling of pages does not have a limit, since the probability of getting banned is very small if we do not brute-force the crawling process and we add some pauses between requests. Since we have access to six different API keys we should be allowed 60,000 requests per day. We expect to get about 5 posts per requests and 1 user profile per requests, which would result in 300,000 posts or 60,000 profiles. Not all of these posts or profiles will end up in our database since they are also subject to a language checker, and may not contain interesting content.
Due to the time constraints of this project we decided to use the API for our crawling tasks since it was easy to access and the request limitation should not hinder the retrieval of posts too much as we will only be presenting a proof-of-concept. However, if we were to develop a real search engine which should contain a larger data collection we would recommend writing a parser that parses the web pages, since these contain more information, some of which is not even supported by the API, and allow for faster data retrieval on a larger scale.

To retrieve the data from the Google+ API a crawler program was written in Python to create the web request, parse the results, check the language and spelling, and then write the data to disk for access by the next program in the pipeline. Since we wanted to label all the retrieved posts with a class we decided to download posts made to a specific Google+ community page. These community pages usually had one specific theme that could be classified into a bigger class, for example: All the posts retrieved from the community page of Samsung could be labeled as 'technology'. We required this labeling for the training and testing of our classifiers, which are introduced in section XXX. 
Since the API is still being developed by Google the documentation does not always correspond with the implementation. We found out that the API was not properly supporting the retrieval of posts from community pages. In order to solve this problem we changed the retrieval of posts from community pages to the result of search queries. The API allows searching for posts containing specific words allowing us to create a list of search words and their corresponding class to label posts accordingly. So instead of retrieving posts from the Samsung community we search for posts containing the word Samsung and label these as 'technology'.
For every post found this way we also retrieve all the corresponding comments and label these the same way. After finding all the posts and their corresponding comments using a specified search word we retrieve all the profiles of the authors, and label these authors with the same label as the posts used to find this author. This way we can also classify authors based on their profile data. We noticed that the profile data provided by users was very diverse, sometimes all the data was provided and other times only a name and gender. This could cause problems for the classification process.

\textbf{Generated output}
For the classifiers to be trained and tested the output of the crawler is written to disk, which allows for easy filtering and separating of the data retrieved. Since we do not require all the data retrieved from the API when training and testing of the classifiers we apply some filtering within the crawler. For posts we are only interested in the user, the original content of the posts, a cleaned version of the content which does not include urls and smileys and we also store the derived sentiment using the technique explained in section XXX. All the posts are stored in a JSON format such that the data can easily be imported by the classifiers. The same process is used for storing profile data.  An example of a post stored in JSON format: 
